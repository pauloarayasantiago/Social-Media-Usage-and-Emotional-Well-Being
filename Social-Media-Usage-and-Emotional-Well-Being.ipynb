{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Project: Social Media Usage and Emotional Well-Being\n",
    "\n",
    "This notebook presents a comprehensive analysis of three datasets (`train.csv`, `val.csv`, and `test.csv`) capturing information on social media usage and users' dominant emotional states. The dataset, meticulously researched and prepared by AI Inventor Emirhan BULUT, explores the relationship between social media usage patterns and emotional well-being.\n",
    "\n",
    "## Dataset Features\n",
    "\n",
    "- **User_ID**: Unique identifier for the user.\n",
    "- **Age**: Age of the user.\n",
    "- **Gender**: Gender of the user (Female, Male, Non-binary).\n",
    "- **Platform**: Social media platform used (e.g., Instagram, Twitter, Facebook, LinkedIn, Snapchat, WhatsApp, Telegram).\n",
    "- **Daily_Usage_Time (minutes)**: Daily time spent on the platform in minutes.\n",
    "- **Posts_Per_Day**: Number of posts made per day.\n",
    "- **Likes_Received_Per_Day**: Number of likes received per day.\n",
    "- **Comments_Received_Per_Day**: Number of comments received per day.\n",
    "- **Messages_Sent_Per_Day**: Number of messages sent per day.\n",
    "- **Dominant_Emotion**: User's dominant emotional state during the day (e.g., Happiness, Sadness, Anger, Anxiety, Boredom, Neutral).\n",
    "\n",
    "## Files\n",
    "\n",
    "- **train.csv**: Data for training models.\n",
    "- **test.csv**: Data for testing models.\n",
    "- **val.csv**: Data for validation purposes.\n",
    "\n",
    "## Usage\n",
    "\n",
    "This dataset can be used for various analyses, including but not limited to:\n",
    "\n",
    "- Predicting users' emotional well-being based on their social media usage.\n",
    "- Clustering users based on usage patterns.\n",
    "- Examining the impact of different platforms on users' emotions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Citation\n",
    "\n",
    "This dataset was meticulously researched and prepared by AI Inventor Emirhan BULUT. If you use this dataset in your work, please cite or reference it as follows:\n",
    "\n",
    "BULUT, E. \"Social Media Usage and Emotional Well-Being Dataset.\" (https://www.kaggle.com/datasets/emirhanai/social-media-usage-and-emotional-well-being).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "First, we need to load the datasets into pandas DataFrames. This will allow us to manipulate and analyze the data effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to load CSV and handle errors\n",
    "def load_csv_with_error_handling(filepath):\n",
    "    try:\n",
    "        return pd.read_csv(filepath)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error reading {filepath}. Trying with on_bad_lines='skip'.\")\n",
    "        return pd.read_csv(filepath, on_bad_lines='skip')\n",
    "\n",
    "# Load the datasets with error handling\n",
    "train_df = load_csv_with_error_handling('train.csv')\n",
    "val_df = load_csv_with_error_handling('val.csv')\n",
    "test_df = load_csv_with_error_handling('test.csv')\n",
    "\n",
    "# Define the function to remove outliers\n",
    "def remove_outliers(df):\n",
    "    return df[~df['Platform'].isin(['55', 'SS'])]\n",
    "\n",
    "# Apply the function to each dataframe\n",
    "train_df = remove_outliers(train_df)\n",
    "val_df = remove_outliers(val_df)\n",
    "test_df = remove_outliers(test_df)\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "display(train_df.head())\n",
    "display(val_df.head())\n",
    "display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "We will now take a look at the basic structure and summary statistics of the datasets. This includes checking the data types, non-null counts, and basic descriptive statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(\"Train Dataset Columns:\")\n",
    "print(train_df.columns)\n",
    "print(\"Validation Dataset Columns:\")\n",
    "print(val_df.columns)\n",
    "print(\"Test Dataset Columns:\")\n",
    "print(test_df.columns)\n",
    "\n",
    "# Basic information about the datasets\n",
    "print(\"Train Dataset Info:\")\n",
    "train_df.info()\n",
    "print(\"\\nValidation Dataset Info:\")\n",
    "val_df.info()\n",
    "print(\"\\nTest Dataset Info:\")\n",
    "test_df.info()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Train Dataset Statistics:\")\n",
    "display(train_df.describe())\n",
    "print(\"Validation Dataset Statistics:\")\n",
    "display(val_df.describe())\n",
    "print(\"Test Dataset Statistics:\")\n",
    "display(test_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Next, we will check for and handle any missing values in the datasets. This step is crucial to ensure the quality and accuracy of our analysis and models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns and convert to numeric, coercing errors to NaN\n",
    "numeric_cols = ['Age', 'Daily_Usage_Time (minutes)', 'Posts_Per_Day', 'Likes_Received_Per_Day', 'Comments_Received_Per_Day', 'Messages_Sent_Per_Day']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
    "    if col in val_df.columns:\n",
    "        val_df[col] = pd.to_numeric(val_df[col], errors='coerce')\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
    "\n",
    "# Fill missing values for numeric columns with median\n",
    "train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\n",
    "val_df[numeric_cols] = val_df[numeric_cols].fillna(val_df[numeric_cols].median())\n",
    "test_df[numeric_cols] = test_df[numeric_cols].fillna(test_df[numeric_cols].median())\n",
    "\n",
    "# Fill missing values for categorical columns with mode\n",
    "categorical_cols = ['User_ID', 'Gender', 'Platform', 'Dominant_Emotion']\n",
    "\n",
    "train_df[categorical_cols] = train_df[categorical_cols].fillna(train_df[categorical_cols].mode().iloc[0])\n",
    "val_df[categorical_cols] = val_df[categorical_cols].fillna(val_df[categorical_cols].mode().iloc[0])\n",
    "test_df[categorical_cols] = test_df[categorical_cols].fillna(test_df[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values in Train Dataset after cleaning:\")\n",
    "display(train_df.isnull().sum())\n",
    "print(\"Missing values in Validation Dataset after cleaning:\")\n",
    "display(val_df.isnull().sum())\n",
    "print(\"Missing values in Test Dataset after cleaning:\")\n",
    "display(test_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we will visualize the data to gain insights into the distribution of different features and the relationships between them. This step helps in understanding the underlying patterns and trends in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example visualizations\n",
    "# Distribution of Daily Usage Time\n",
    "plt.hist(train_df['Daily_Usage_Time (minutes)'], bins=30)\n",
    "plt.title('Distribution of Daily Usage Time')\n",
    "plt.xlabel('Daily Usage Time (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Daily Usage Time\n",
    "\n",
    "The histogram above shows the distribution of the daily usage time of social media platforms among users. It helps us understand how much time users typically spend on social media each day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Dominant Emotions\n",
    "train_df['Dominant_Emotion'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Dominant Emotions')\n",
    "plt.xlabel('Dominant Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Dominant Emotions\n",
    "\n",
    "The bar chart above illustrates the frequency of different dominant emotional states among users. This visualization gives us an overview of the prevalent emotional states in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate correlation matrix using only numeric columns\n",
    "numeric_train_df = train_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Ensure all columns are numeric and drop any rows with NaN values that might remain\n",
    "numeric_train_df = numeric_train_df.dropna()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_train_df.corr()\n",
    "\n",
    "# Plot the correlation matrix with annotations using matplotlib directly\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = plt.pcolor(correlation_matrix, cmap='viridis')\n",
    "\n",
    "plt.colorbar(heatmap)\n",
    "plt.xticks(np.arange(0.5, len(correlation_matrix.columns), 1), correlation_matrix.columns, rotation=45)\n",
    "plt.yticks(np.arange(0.5, len(correlation_matrix.index), 1), correlation_matrix.index)\n",
    "\n",
    "# Adding annotations\n",
    "for y in range(correlation_matrix.shape[0]):\n",
    "    for x in range(correlation_matrix.shape[1]):\n",
    "        plt.text(x + 0.5, y + 0.5, f'{correlation_matrix.iloc[y, x]:.2f}',\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 color='white')\n",
    "\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Before building models, we need to prepare the data by defining our features (input variables) and the target variable (output). We will also split the data into training and validation sets if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['User_ID', 'Dominant_Emotion', 'Gender', 'Platform'])\n",
    "y_train = train_df['Dominant_Emotion']\n",
    "X_val = val_df.drop(columns=['User_ID', 'Dominant_Emotion', 'Gender', 'Platform'])\n",
    "y_val = val_df['Dominant_Emotion']\n",
    "X_test = test_df.drop(columns=['User_ID', 'Dominant_Emotion', 'Gender', 'Platform'])\n",
    "y_test = test_df['Dominant_Emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling evaluation with FLAML\n",
    "\n",
    "We will use FLAML (Fast and Lightweight AutoML) to automatically select the best model and hyperparameters for predicting the dominant emotional state of users based on their social media usage. FLAML provides an efficient and effective way to perform automated machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "\n",
    "# Define the AutoML model\n",
    "autoML = AutoML()\n",
    "\n",
    "# Specify settings\n",
    "settings = {\n",
    "    \"time_budget\": 30,  # Total running time in seconds\n",
    "    \"metric\": 'roc_auc_ovo',  # Primary metric\n",
    "    \"task\": 'classification',  # Task type\n",
    "    \"log_file_name\": \"flaml.log\",  # Log file name\n",
    "}\n",
    "\n",
    "# Train the AutoML model\n",
    "autoML.fit(X_train=X_train, y_train=y_train, **settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best AutoML Model: {autoML.best_estimator}\\n')\n",
    "print(f'Best Paramter AutoML Model:\\n {autoML.best_config}\\n')\n",
    "print(f'Best roc_auc_ovo On Val data: {1 - autoML.best_loss:.4g}\\n')\n",
    "print(f'Best Run Training duration: {autoML.best_config_train_time:.4g} s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=((20,15)), facecolor='yellow')  # Set background color to yellow\n",
    "plt.barh(\n",
    "    autoML.model.estimator.feature_name_, autoML.model.estimator.feature_importances_\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with FLAML\n",
    "\n",
    "We will evaluate the best model found by FLAML on the validation set to assess its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = autoML.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "final_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Final Accuracy:\", final_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal model according to FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Extract the best model and its configuration\n",
    "best_model = autoML.model\n",
    "best_config = autoML.best_config\n",
    "\n",
    "# Recreate the model using the best configuration\n",
    "if best_model.estimator_class == xgb.XGBClassifier:\n",
    "    recreated_model = xgb.XGBClassifier(**best_config)\n",
    "else:\n",
    "    recreated_model = best_model.estimator_class(**best_config)\n",
    "\n",
    "# Fit the recreated model\n",
    "recreated_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the recreated model\n",
    "y_val_pred_recreated = recreated_model.predict(X_val)\n",
    "recreated_model_accuracy = accuracy_score(y_val, y_val_pred_recreated)\n",
    "\n",
    "print(f\"Best AutoML Model: {autoML.best_estimator}\")\n",
    "print(f\"Best Configuration: {best_config}\")\n",
    "print(f\"Recreated Model Accuracy: {recreated_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = recreated_model.predict(X_test)\n",
    "\n",
    "# Mapping emotions to colors\n",
    "emotion_color_map = {\n",
    "    'Neutral': 'blue',\n",
    "    'Anxiety': 'red',\n",
    "    'Happiness': 'green',\n",
    "    'Boredom': 'purple',\n",
    "    'Sadness': 'orange',\n",
    "    'Anger': 'brown'\n",
    "}\n",
    "\n",
    "# Convert predicted labels to colors\n",
    "y_test_pred_colors = np.array([emotion_color_map[emotion] for emotion in y_test_pred])\n",
    "\n",
    "# List of features to plot on the y-axis\n",
    "features_to_plot = ['Posts_Per_Day', 'Likes_Received_Per_Day', 'Comments_Received_Per_Day', 'Messages_Sent_Per_Day']\n",
    "\n",
    "# Create subplots in a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10), constrained_layout=True)\n",
    "\n",
    "# Generate scatter plots with Daily Usage Time on the x-axis and other features on the y-axis\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    scatter = ax.scatter(X_test['Daily_Usage_Time (minutes)'], X_test[feature], c=y_test_pred_colors, alpha=0.7)\n",
    "    ax.set_title(f'Predicted Emotional Well-Being vs {feature}')\n",
    "    ax.set_xlabel('Daily Usage Time (minutes)')\n",
    "    ax.set_ylabel(feature)\n",
    "\n",
    "# Create a custom legend\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=emotion) for emotion, color in emotion_color_map.items()]\n",
    "fig.legend(handles=handles, title=\"Emotional States\", loc='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Insights\n",
    "\n",
    "- Higher daily usage time is generally associated with increased social media activity (posts, likes, comments, messages).\n",
    "- Positive emotional states, particularly happiness, are more frequently predicted among users who are more active on social media.\n",
    "- Negative emotional states (such as anxiety and sadness) are less distinctly clustered around higher activity metrics, indicating that higher activity levels might be linked with positive emotional well-being.\n",
    "- These visualizations suggest that users with higher engagement on social media platforms (likes, comments, and messages) tend to have a better emotional state as predicted by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming train_df, val_df, and test_df are already cleaned\n",
    "# Combine the datasets if needed or use only the training set for clustering\n",
    "combined_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['Daily_Usage_Time (minutes)', 'Posts_Per_Day', 'Likes_Received_Per_Day', 'Comments_Received_Per_Day', 'Messages_Sent_Per_Day']\n",
    "X = combined_df[features]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Elbow Method\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), sse, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores for Different Clusters')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with the optimal number of clusters\n",
    "optimal_clusters = 4\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "combined_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Add cluster labels to the original dataframe for analysis\n",
    "combined_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Profile each cluster\n",
    "cluster_profiles = combined_df.groupby('Cluster')[features].mean()\n",
    "print(cluster_profiles)\n",
    "\n",
    "# Evaluate cluster quality\n",
    "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "print(f'Silhouette Score: {silhouette_avg}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use PCA to reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Define colors for clusters\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "cluster_labels_unique = np.unique(cluster_labels)\n",
    "\n",
    "# Plot the clusters with a legend\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster in cluster_labels_unique:\n",
    "    plt.scatter(X_pca[cluster_labels == cluster, 0], X_pca[cluster_labels == cluster, 1], \n",
    "                label=f'Cluster {cluster} - {colors[cluster]}', color=colors[cluster], alpha=0.6)\n",
    "\n",
    "plt.title('Clusters of Users Based on Social Media Usage Patterns')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title=\"Clusters\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'combined_df' is the DataFrame that contains the cluster labels and emotions\n",
    "emotion_distribution = combined_df.groupby(['Cluster', 'Dominant_Emotion']).size().unstack(fill_value=0)\n",
    "print(emotion_distribution)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the emotion distribution for each cluster\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10), constrained_layout=True)\n",
    "\n",
    "# Define the clusters and colors for each emotion\n",
    "clusters = [0, 1, 2, 3]\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "# Create bar plots for each cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    emotion_counts = emotion_distribution.loc[cluster]\n",
    "    bars = ax.bar(emotion_counts.index, emotion_counts.values, color=colors[i])\n",
    "    ax.set_title(f'Emotion Distribution in Cluster {cluster}')\n",
    "    ax.set_xlabel('Emotion')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xticklabels(emotion_counts.index, rotation=45)\n",
    "\n",
    "    # Label bars with the counts\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Emotion Distribution Across Clusters', fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Characteristics and Emotion Distribution\n",
    "\n",
    "#### Cluster 0 (Blue):\n",
    "- **Average Daily Usage Time**: 50 minutes\n",
    "- **Average Posts Per Day**: 1\n",
    "- **Average Likes Received Per Day**: 10\n",
    "- **Average Comments Received Per Day**: 5\n",
    "- **Average Messages Sent Per Day**: 3\n",
    "\n",
    "*Analysis*: Users in this cluster have lower daily usage time and social media activity, indicating they are less engaged.\n",
    "\n",
    "*Emotion Distribution Assumptions*:\n",
    "- This cluster is predominantly associated with neutral emotions and boredom.\n",
    "- There is a significant presence of anxiety and sadness, indicating that less engaged users might experience more negative emotions.\n",
    "\n",
    "\n",
    "#### Cluster 1 (Orange):\n",
    "- **Average Daily Usage Time**: 90 minutes\n",
    "- **Average Posts Per Day**: 3\n",
    "- **Average Likes Received Per Day**: 30\n",
    "- **Average Comments Received Per Day**: 15\n",
    "- **Average Messages Sent Per Day**: 10\n",
    "\n",
    "*Analysis*: Users in this cluster exhibit moderate social media activity and usage patterns, representing average engagement.\n",
    "\n",
    "*Emotion Distribution Assumptions*:\n",
    "- This cluster shows a high occurrence of happiness, suggesting that moderately engaged users tend to be happier.\n",
    "- There are also instances of anger, anxiety, and sadness, but they are less prevalent compared to happiness.\n",
    "\n",
    "\n",
    "#### Cluster 2 (Green):\n",
    "- **Average Daily Usage Time**: 130 minutes\n",
    "- **Average Posts Per Day**: 5\n",
    "- **Average Likes Received Per Day**: 50\n",
    "- **Average Comments Received Per Day**: 25\n",
    "- **Average Messages Sent Per Day**: 20\n",
    "\n",
    "*Analysis*: This group includes users with relatively high social media usage and engagement, indicating they are highly active.\n",
    "\n",
    "*Emotion Distribution Assumptions*:\n",
    "- The predominant emotion in this cluster is happiness, indicating that highly engaged users are mostly happy.\n",
    "- Negative emotions like anxiety are minimally present.\n",
    "\n",
    "\n",
    "#### Cluster 3 (Red):\n",
    "- **Average Daily Usage Time**: 180 minutes\n",
    "- **Average Posts Per Day**: 8\n",
    "- **Average Likes Received Per Day**: 80\n",
    "- **Average Comments Received Per Day**: 35\n",
    "- **Average Messages Sent Per Day**: 30\n",
    "\n",
    "*Analysis*: Users in this cluster show the highest levels of social media activity and daily usage time, representing the most engaged users.\n",
    "\n",
    "*Emotion Distribution Assumptions*:\n",
    "- This cluster has a diverse emotional distribution with significant occurrences of anger, anxiety, and sadness.\n",
    "- Despite high engagement, these users do not predominantly experience positive emotions, indicating a possible link between high engagement and negative emotional states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution across platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distribution of emotions for each platform\n",
    "platform_emotion_distribution = combined_df.groupby(['Platform', 'Dominant_Emotion']).size().unstack(fill_value=0)\n",
    "print(platform_emotion_distribution)\n",
    "\n",
    "# Define platforms\n",
    "platforms = platform_emotion_distribution.index\n",
    "\n",
    "# Create bar plots for each platform in a 2x4 grid layout\n",
    "fig, axs = plt.subplots(4, 2, figsize=(15, 20), constrained_layout=True)\n",
    "\n",
    "for i, platform in enumerate(platforms):\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    emotion_counts = platform_emotion_distribution.loc[platform]\n",
    "    bars = ax.bar(emotion_counts.index, emotion_counts.values)\n",
    "    ax.set_title(f'Emotion Distribution on {platform}')\n",
    "    ax.set_xlabel('Emotion')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xticklabels(emotion_counts.index, rotation=45)\n",
    "\n",
    "    # Label bars with the counts\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Emotion Distribution Across Platforms', fontsize=16)\n",
    "plt.savefig('emotion_distribution_across_platforms.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Impact of Different Platforms on Users' Emotions\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "- **Facebook**: High in neutral and anxiety; moderate boredom and sadness.\n",
    "- **Instagram**: High happiness; moderate anxiety, neutral, and sadness; low anger.\n",
    "- **LinkedIn**: High boredom; moderate neutral and anxiety; low sadness and anger.\n",
    "- **Snapchat**: High anxiety; moderate neutral and happiness.\n",
    "- **Telegram**: High neutral and sadness; moderate anger and anxiety.\n",
    "- **Twitter**: High anger and sadness; moderate neutral and anxiety; low happiness.\n",
    "- **WhatsApp**: High anger; moderate neutral and anxiety; low happiness.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
